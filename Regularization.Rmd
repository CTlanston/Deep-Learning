---
title: "Homework 1"
author: "Lanston"
output:
  pdf_document: default
  word_document: default
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### 1. What is the main objective of Lasso regression?


 a) To shrink the coefficients of the model towards zero
 b) To perform feature selection by forcing some coefficients to be exactly equal to zero
 c) To minimize the residual sum of squares
 d) To maximize R-squared

Answer 1: b)

### 2. What is the main difference between Ridge and Lasso regression?


 a) Ridge regression shrinks the coefficients of the model towards zero, while Lasso regression forces some coefficients to be exactly equal to zero
 b) Lasso regression shrinks the coefficients of the model towards zero, while Ridge regression forces some coefficients to be exactly equal to zero
 c) Both Ridge and Lasso regression shrink the coefficients of the model towards zero
 d) Both Ridge and Lasso regression force some coefficients to be exactly equal to zero


Answer 2: a)


### 3. What is the main advantage of splitting the data into training and testing sets when building a linear regression model?
 a) It allows us to evaluate the model's performance on new, unseen data
 b) It allows us to estimate the model's true error, which is the error that will be made when the model is used to make predictions on new data
 c) It prevents overfitting such that the fitted model that has a high accuracy on the given data but performs poorly on new, unseen data
 d) All of the above


Answer 3:d)

# 4. Boston housing data 
```{r}
Boston <- read.csv('https://zhang-datasets.s3.us-east-2.amazonaws.com/Boston.csv')
```

### (a) Fit a simple linear regression that predicts _medv_ (median house value) using _lstat_ (percent households with low socioeconomic status). Use this model to answer the following questions.
```{r}
lm1 <-lm(medv~lstat,data=Boston)
summary(lm1)
```

### From the p_values, is _lstat_ a significant predictor for _medv_? What is the R-squared of this linear regression?
Answer: yes, the p_value is significantly less than 5%

### What happens to _medv_ with one precent increase in lstat?
Answer: one percent increase in lstat, the the medv will descrise by 1%*-0.95005%

### (b) Predict _medv_ for _lstat_ = 5, 10, 15, respectively.
```{r}
new_lstat <-data.frame(lstat=c(5,10,15))
predict(lm1,new_lstat)
```

### (c) Fit a multiple linear regression that predicts _medv_ using all the covariates in the data set. Use this model to answer the following questions.
```{r}
lm2 <- lm(medv~.,data = Boston)
summary(lm2)
```
### Which covariates are significant? 
Answer: instead of indus and age, all the covariates are significant 

### What is the R-squared now?
Answer: the multiple R-squared are 0.0734  and adjusted R-squared are 0.7278

### (d) We will now try to predict per capita crime rate by town (the column _crim_) in the Boston data set. Considering all predictors in this data set, try out the standard linear regression, ridge regression and lasso regression. Propose a model that performs the best on this data set, and justify your answer using evidence from model fitting. (Note: make sure that you evaluate model performance using testing error, as opposed to training error.)
```{r}
library(glmnet)
library(car)
grid = 10^seq(10,-2,length=100)
x = model.matrix(crim~.,Boston)
y = Boston$crim

set.seed(1)

dim(Boston)

train <- sample(1:nrow(x),nrow(x)/2)
test <- setdiff(1:nrow(x), train)

lm3 <- lm(crim~.,data = Boston, subset = train)
lm_lasso <- glmnet(x[train,],y[train], alpha=1,lambda=grid)
lm_ridge <- glmnet(x[train,],y[train], alpha=0,lambda=grid)

#calculate the min lambda 
cv.out1 <- cv.glmnet(x[train,],y[train],alpha=1) 
best_lambda_lasso <- cv.out1$lambda.min 
best_lambda_lasso 
cv.out2 <- cv.glmnet(x[train,],y[train],alpha=0) 
best_lambda_rige <- cv.out2$lambda.min 
best_lambda_rige

#make the prediction 
lm3.pred <- predict(lm3,newdata = Boston[test,])
lm_lasso.pred <- predict(lm_lasso,s=0.003893966,newx = x[test,])
lm_ridge.pred <- predict(lm_ridge,s=0.502925,newx = x[test,])

#mse calculation
mse_lm <- mean((y[test]-lm3.pred)^2)
mse_lasso <- mean((y[test]-lm_lasso.pred)^2)
mse_ridge <- mean((y[test]-lm_ridge.pred)^2)

```

```{r}
#compare the 3 kinds of models 
print(list(lm = mse_lm, lasso = mse_lasso, ridge = mse_ridge))
summary(lm3)
vif(lm3)
```
# answers for comparing 3 kinds of model
### 1.from the mse, the ridge regression model is better. 
### 2.as we can see the summary of standard linear regression, there are several variables are significant, so ridge is better which make sense
### 3.use VIF value we can see that tax and rad which vif value is 7 and 9 which bigger than 5, means there are collinearity in standard linear regression model, so ridge regression is better. 

